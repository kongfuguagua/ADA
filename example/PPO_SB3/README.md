# PPO_SB3 - åŸºäº Stable Baselines3 çš„ PPO æ™ºèƒ½ä½“

## ğŸ“– é¡¹ç›®ç®€ä»‹

PPO_SB3 æ˜¯ä¸€ä¸ªåŸºäº **Stable Baselines3** æ¡†æ¶å®ç°çš„ **PPO (Proximal Policy Optimization)** å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œä¸“é—¨ç”¨äºåœ¨ **Grid2Op** ç”µåŠ›ç³»ç»Ÿç¯å¢ƒä¸­è¿›è¡Œè¿ç»­åŠ¨ä½œæ§åˆ¶ã€‚

æœ¬é¡¹ç›®æ˜¯ L2RPN (Learning to Run a Power Network) ç«èµ›çš„åŸºçº¿å®ç°ä¹‹ä¸€ï¼Œä¸»è¦ç”¨äºç ”ç©¶è¿ç»­åŠ¨ä½œå¯¹ç”µåŠ›ç³»ç»Ÿçš„å½±å“ï¼ŒåŒ…æ‹¬ï¼š

- **å‚¨èƒ½å•å…ƒ (Storage Units)** çš„å……æ”¾ç”µæ§åˆ¶
- **å¯è°ƒåº¦å‘ç”µæœº (Dispatchable Generators)** çš„é‡æ–°è°ƒåº¦ (Redispatch)
- **å¯å†ç”Ÿèƒ½æºå‘ç”µæœº** çš„å‰Šå‡æ§åˆ¶ (Curtailment)

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **è¿ç»­åŠ¨ä½œç©ºé—´**ï¼šä¸“æ³¨äºè¿ç»­å˜é‡æ§åˆ¶ï¼Œé€‚åˆç”µåŠ›ç³»ç»Ÿçš„ç²¾ç»†è°ƒèŠ‚
- ğŸ”„ **Gym å…¼å®¹**ï¼šä½¿ç”¨ Grid2Op çš„ `gym_compat` æ¨¡å—ï¼Œå°† Grid2Op ç¯å¢ƒè½¬æ¢ä¸ºæ ‡å‡† Gym ç¯å¢ƒ
- ğŸ“Š **TensorBoard æ”¯æŒ**ï¼šå®Œæ•´çš„è®­ç»ƒæ—¥å¿—è®°å½•ï¼Œä¾¿äºç›‘æ§å’Œè°ƒè¯•
- ğŸ’¾ **æ¨¡å‹æ£€æŸ¥ç‚¹**ï¼šæ”¯æŒè®­ç»ƒè¿‡ç¨‹ä¸­çš„è‡ªåŠ¨ä¿å­˜å’Œæ¢å¤
- ğŸ” **è¯„ä¼°å›è°ƒ**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½
- ğŸ›ï¸ **çµæ´»é…ç½®**ï¼šå¯è‡ªå®šä¹‰è§‚å¯Ÿç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œç½‘ç»œæ¶æ„
- ğŸ“ˆ **å½’ä¸€åŒ–æ”¯æŒ**ï¼šæ”¯æŒè§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´çš„è‡ªåŠ¨å½’ä¸€åŒ–

## ğŸ“¦ ä¾èµ–è¦æ±‚

### å¿…éœ€ä¾èµ–

```bash
# æ ¸å¿ƒä¾èµ–
grid2op>=1.7.0
stable-baselines3>=1.5.0
lightsim2grid>=0.7.0  # å¼ºçƒˆæ¨èï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒ

# å¯é€‰ä½†æ¨è
tensorboard  # ç”¨äºå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
```

### å®‰è£…æ–¹æ³•

```bash
# å®‰è£… l2rpn-baselinesï¼ˆåŒ…å« PPO_SB3ï¼‰
pip install l2rpn-baselines

# æˆ–ä»æºç å®‰è£…
git clone https://github.com/rte-france/l2rpn-baselines.git
cd l2rpn-baselines
pip install -e .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬è®­ç»ƒç¤ºä¾‹

```python
import re
import grid2op
from grid2op.Reward import LinesCapacityReward
from grid2op.Chronics import MultifolderWithCache
from lightsim2grid import LightSimBackend
from l2rpn_baselines.PPO_SB3 import train

# åˆ›å»ºç¯å¢ƒ
env_name = "l2rpn_case14_sandbox"
env = grid2op.make(
    env_name,
    reward_class=LinesCapacityReward,
    backend=LightSimBackend(),  # ä½¿ç”¨ LightSimBackend åŠ é€Ÿ
    chronics_class=MultifolderWithCache  # ç¼“å­˜æ•°æ®ä»¥åŠ é€Ÿè®­ç»ƒ
)

# è¿‡æ»¤è®­ç»ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
env.chronics_handler.real_data.set_filter(
    lambda x: re.match(".*00$", x) is not None
)
env.chronics_handler.real_data.reset()

try:
    # è®­ç»ƒæ™ºèƒ½ä½“
    trained_agent = train(
        env,
        iterations=10_000,  # è®­ç»ƒæ­¥æ•°
        logs_dir="./logs",  # TensorBoard æ—¥å¿—ç›®å½•
        save_path="./saved_model",  # æ¨¡å‹ä¿å­˜è·¯å¾„
        name="my_ppo_agent",  # æ¨¡å‹åç§°
        net_arch=[200, 200, 200],  # ç¥ç»ç½‘ç»œæ¶æ„ [éšè—å±‚1, éšè—å±‚2, éšè—å±‚3]
        save_every_xxx_steps=2000,  # æ¯ 2000 æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        eval_every_xxx_steps=1000,  # æ¯ 1000 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼ˆéœ€è¦æä¾› eval_envï¼‰
    )
finally:
    env.close()
```

### 2. å¸¦è¯„ä¼°ç¯å¢ƒçš„è®­ç»ƒ

```python
import grid2op
from grid2op.Reward import LinesCapacityReward
from lightsim2grid import LightSimBackend
from grid2op.Chronics import MultifolderWithCache
from l2rpn_baselines.PPO_SB3 import train

# åˆ›å»ºè®­ç»ƒç¯å¢ƒ
env = grid2op.make(
    "l2rpn_case14_sandbox",
    reward_class=LinesCapacityReward,
    backend=LightSimBackend(),
    chronics_class=MultifolderWithCache
)

# åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
eval_env = grid2op.make(
    "l2rpn_case14_sandbox",
    reward_class=LinesCapacityReward,
    backend=LightSimBackend(),
    chronics_class=MultifolderWithCache,
    test=True  # ä½¿ç”¨æµ‹è¯•é›†
)

try:
    trained_agent = train(
        env,
        iterations=10_000,
        logs_dir="./logs",
        save_path="./saved_model",
        name="my_ppo_agent",
        net_arch=[200, 200, 200],
        save_every_xxx_steps=2000,
        eval_every_xxx_steps=1000,  # å¯ç”¨è¯„ä¼°å›è°ƒ
        eval_env=eval_env,  # æä¾›è¯„ä¼°ç¯å¢ƒ
    )
finally:
    env.close()
    eval_env.close()
```

### 3. è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹

```python
import grid2op
from grid2op.Reward import LinesCapacityReward
from lightsim2grid import LightSimBackend
from l2rpn_baselines.PPO_SB3 import evaluate

# åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
env = grid2op.make(
    "l2rpn_case14_sandbox",
    reward_class=LinesCapacityReward,
    backend=LightSimBackend()
)

try:
    # è¯„ä¼°æ™ºèƒ½ä½“
    trained_agent, results = evaluate(
        env,
        load_path="./saved_model",  # æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        name="my_ppo_agent",  # æ¨¡å‹åç§°ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        nb_episode=7,  # è¯„ä¼°çš„å›åˆæ•°
        nb_process=1,  # å¹¶è¡Œè¿›ç¨‹æ•°
        logs_path="./logs-eval",  # è¯„ä¼°æ—¥å¿—ä¿å­˜è·¯å¾„
        verbose=True,  # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        save_gif=False,  # æ˜¯å¦ä¿å­˜ GIFï¼ˆå¯èƒ½å ç”¨å¤§é‡å†…å­˜ï¼‰
    )
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("è¯„ä¼°å®Œæˆï¼")
    for _, chron_name, cum_reward, nb_time_step, max_ts in results:
        print(f"åœºæ™¯: {chron_name}")
        print(f"  æ€»å¥–åŠ±: {cum_reward:.6f}")
        print(f"  æ­¥æ•°: {nb_time_step}/{max_ts}")
finally:
    env.close()
```

## ğŸ“ ä»£ç ç»“æ„

```
PPO_SB3/
â”œâ”€â”€ __init__.py          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ train.py             # è®­ç»ƒå‡½æ•°
â”œâ”€â”€ evaluate.py          # è¯„ä¼°å‡½æ•°
â””â”€â”€ utils.py             # å·¥å…·å‡½æ•°å’Œ SB3Agent ç±»
```

### ä¸»è¦æ¨¡å—è¯´æ˜

#### `train.py`
- **`train()`**: ä¸»è®­ç»ƒå‡½æ•°ï¼Œè´Ÿè´£åˆ›å»º Gym ç¯å¢ƒã€åˆå§‹åŒ– PPO æ¨¡å‹ã€è®­ç»ƒå’Œä¿å­˜
- **`build_gym_env()`**: å°† Grid2Op ç¯å¢ƒè½¬æ¢ä¸º Gym ç¯å¢ƒ

#### `evaluate.py`
- **`evaluate()`**: è¯„ä¼°å‡½æ•°ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶åœ¨ç¯å¢ƒä¸­è¿è¡Œè¯„ä¼°

#### `utils.py`
- **`SB3Agent`**: æ ¸å¿ƒæ™ºèƒ½ä½“ç±»ï¼Œå°è£…äº† Stable Baselines3 çš„ PPO æ¨¡å‹
- **`default_obs_attr_to_keep`**: é»˜è®¤è§‚å¯Ÿå±æ€§åˆ—è¡¨
- **`default_act_attr_to_keep`**: é»˜è®¤åŠ¨ä½œå±æ€§åˆ—è¡¨
- **`remove_non_usable_attr()`**: ç§»é™¤ä¸å¯ç”¨çš„åŠ¨ä½œå±æ€§
- **`save_used_attribute()`**: ä¿å­˜ä½¿ç”¨çš„è§‚å¯Ÿå’ŒåŠ¨ä½œå±æ€§

## ğŸ”§ ä¸»è¦å‚æ•°è¯¦è§£

### è®­ç»ƒå‚æ•° (`train()`)

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `env` | `grid2op.Environment` | - | Grid2Op ç¯å¢ƒï¼ˆå¿…éœ€ï¼‰ |
| `name` | `str` | `"PPO_SB3"` | æ¨¡å‹åç§° |
| `iterations` | `int` | `1` | è®­ç»ƒæ­¥æ•°ï¼ˆä¸æ˜¯å›åˆæ•°ï¼‰ |
| `save_path` | `str` | `None` | æ¨¡å‹ä¿å­˜è·¯å¾„ |
| `load_path` | `str` | `None` | æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆç”¨äºç»§ç»­è®­ç»ƒï¼‰ |
| `net_arch` | `list` | `None` | ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¦‚ `[200, 200, 200]` |
| `logs_dir` | `str` | `None` | TensorBoard æ—¥å¿—ç›®å½• |
| `learning_rate` | `float` | `3e-4` | å­¦ä¹ ç‡ |
| `save_every_xxx_steps` | `int` | `None` | æ¯ N æ­¥ä¿å­˜æ£€æŸ¥ç‚¹ |
| `eval_every_xxx_steps` | `int` | `None` | æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡ï¼ˆéœ€æä¾› `eval_env`ï¼‰ |
| `obs_attr_to_keep` | `list` | é»˜è®¤åˆ—è¡¨ | ä¿ç•™çš„è§‚å¯Ÿå±æ€§ |
| `act_attr_to_keep` | `list` | é»˜è®¤åˆ—è¡¨ | ä¿ç•™çš„åŠ¨ä½œå±æ€§ |
| `normalize_obs` | `bool` | `False` | æ˜¯å¦å½’ä¸€åŒ–è§‚å¯Ÿç©ºé—´ |
| `normalize_act` | `bool` | `False` | æ˜¯å¦å½’ä¸€åŒ–åŠ¨ä½œç©ºé—´ |
| `eval_env` | `grid2op.Environment` | `None` | è¯„ä¼°ç¯å¢ƒï¼ˆç”¨äºè®­ç»ƒæ—¶è¯„ä¼°ï¼‰ |

### è¯„ä¼°å‚æ•° (`evaluate()`)

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `env` | `grid2op.Environment` | - | Grid2Op ç¯å¢ƒï¼ˆå¿…éœ€ï¼‰ |
| `load_path` | `str` | `"."` | æ¨¡å‹åŠ è½½è·¯å¾„ |
| `name` | `str` | `"PPO_SB3"` | æ¨¡å‹åç§°ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰ |
| `nb_episode` | `int` | `1` | è¯„ä¼°å›åˆæ•° |
| `nb_process` | `int` | `1` | å¹¶è¡Œè¿›ç¨‹æ•° |
| `max_steps` | `int` | `-1` | æœ€å¤§æ­¥æ•°ï¼ˆ-1 è¡¨ç¤ºæ— é™åˆ¶ï¼‰ |
| `logs_path` | `str` | `None` | è¯„ä¼°æ—¥å¿—ä¿å­˜è·¯å¾„ |
| `save_gif` | `bool` | `False` | æ˜¯å¦ä¿å­˜ GIF åŠ¨ç”» |
| `iter_num` | `int` | `None` | åŠ è½½ç‰¹å®šè®­ç»ƒè¿­ä»£çš„æ¨¡å‹ |

### é»˜è®¤è§‚å¯Ÿå±æ€§

```python
default_obs_attr_to_keep = [
    "day_of_week", "hour_of_day", "minute_of_hour",  # æ—¶é—´ä¿¡æ¯
    "prod_p", "prod_v",  # å‘ç”µåŠŸç‡å’Œç”µå‹
    "load_p", "load_q",  # è´Ÿè·åŠŸç‡
    "actual_dispatch", "target_dispatch",  # è°ƒåº¦ä¿¡æ¯
    "topo_vect",  # æ‹“æ‰‘å‘é‡
    "time_before_cooldown_line", "time_before_cooldown_sub",  # å†·å´æ—¶é—´
    "rho",  # çº¿è·¯è´Ÿè½½ç‡
    "timestep_overflow",  # è¿‡è½½æ—¶é—´æ­¥
    "line_status",  # çº¿è·¯çŠ¶æ€
    "storage_power", "storage_charge",  # å‚¨èƒ½ä¿¡æ¯
]
```

### é»˜è®¤åŠ¨ä½œå±æ€§

```python
default_act_attr_to_keep = [
    "redispatch",    # é‡æ–°è°ƒåº¦ï¼ˆå¯è°ƒåº¦å‘ç”µæœºï¼‰
    "curtail",       # å‰Šå‡ï¼ˆå¯å†ç”Ÿèƒ½æºï¼‰
    "set_storage",   # å‚¨èƒ½æ§åˆ¶
]
```

## ğŸ¯ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´

```python
from l2rpn_baselines.PPO_SB3 import train

# è‡ªå®šä¹‰è§‚å¯Ÿå±æ€§
custom_obs_attr = [
    "rho",  # åªä¿ç•™çº¿è·¯è´Ÿè½½ç‡
    "prod_p",  # å‘ç”µåŠŸç‡
    "load_p",  # è´Ÿè·åŠŸç‡
]

# è‡ªå®šä¹‰åŠ¨ä½œå±æ€§
custom_act_attr = [
    "redispatch",  # åªä½¿ç”¨é‡æ–°è°ƒåº¦
    # "curtail",  # ä¸ä½¿ç”¨å‰Šå‡
    # "set_storage",  # ä¸ä½¿ç”¨å‚¨èƒ½
]

trained_agent = train(
    env,
    iterations=10_000,
    save_path="./saved_model",
    name="custom_agent",
    obs_attr_to_keep=custom_obs_attr,
    act_attr_to_keep=custom_act_attr,
    net_arch=[256, 256, 128],  # è‡ªå®šä¹‰ç½‘ç»œæ¶æ„
)
```

### å¯ç”¨å½’ä¸€åŒ–

```python
trained_agent = train(
    env,
    iterations=10_000,
    save_path="./saved_model",
    name="normalized_agent",
    normalize_obs=True,  # å½’ä¸€åŒ–è§‚å¯Ÿç©ºé—´
    normalize_act=True,  # å½’ä¸€åŒ–åŠ¨ä½œç©ºé—´
)
```

### ç»§ç»­è®­ç»ƒï¼ˆä»æ£€æŸ¥ç‚¹æ¢å¤ï¼‰

```python
trained_agent = train(
    env,
    iterations=20_000,  # æ€»è®­ç»ƒæ­¥æ•°
    load_path="./saved_model",  # ä»è¯¥è·¯å¾„åŠ è½½
    save_path="./saved_model",  # ä¿å­˜åˆ°è¯¥è·¯å¾„
    name="my_ppo_agent",
    # å…¶ä»–å‚æ•°ä¼šè‡ªåŠ¨ä»ä¿å­˜çš„é…ç½®ä¸­æ¢å¤
)
```

### ä½¿ç”¨ TensorBoard ç›‘æ§è®­ç»ƒ

```python
# è®­ç»ƒæ—¶æŒ‡å®š logs_dir
trained_agent = train(
    env,
    iterations=10_000,
    logs_dir="./logs",  # TensorBoard æ—¥å¿—ç›®å½•
    save_path="./saved_model",
    name="my_ppo_agent",
)

# å¯åŠ¨ TensorBoard
# åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir=./logs
```

### åŠ è½½ç‰¹å®šè¿­ä»£çš„æ¨¡å‹

```python
# è¯„ä¼°æ—¶æŒ‡å®š iter_num
trained_agent, results = evaluate(
    env,
    load_path="./saved_model",
    name="my_ppo_agent",
    iter_num=8000,  # åŠ è½½ç¬¬ 8000 æ­¥çš„æ¨¡å‹
    nb_episode=7,
)
```

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹è„šæœ¬

é¡¹ç›®æ ¹ç›®å½•çš„ `train.py` å’Œ `evaluate.py` æ–‡ä»¶åŒ…å«å¯ç›´æ¥è¿è¡Œçš„ç¤ºä¾‹ï¼š

### è¿è¡Œè®­ç»ƒç¤ºä¾‹

```bash
# ç›´æ¥è¿è¡Œ train.py
python -m l2rpn_baselines.PPO_SB3.train

# æˆ–ä¿®æ”¹ train.py ä¸­çš„å‚æ•°åè¿è¡Œ
python l2rpn_baselines/PPO_SB3/train.py
```

### è¿è¡Œè¯„ä¼°ç¤ºä¾‹

```bash
# ç›´æ¥è¿è¡Œ evaluate.py
python -m l2rpn_baselines.PPO_SB3.evaluate

# æˆ–ä¿®æ”¹ evaluate.py ä¸­çš„å‚æ•°åè¿è¡Œ
python l2rpn_baselines/PPO_SB3/evaluate.py
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. Gym/Gymnasium å…¼å®¹æ€§

ç›®å‰ Grid2Op ä½¿ç”¨çš„æ˜¯æ—§ç‰ˆ Gym APIï¼Œè€Œ Stable Baselines3 å¯èƒ½ä½¿ç”¨æ–°ç‰ˆ Gymnasium APIã€‚è¿™å¯èƒ½å¯¼è‡´å…¼å®¹æ€§é—®é¢˜ã€‚å»ºè®®ï¼š

- ä½¿ç”¨å…¼å®¹ç‰ˆæœ¬çš„ `stable-baselines3`
- å…³æ³¨ Grid2Op çš„æ›´æ–°ï¼Œæœªæ¥å¯èƒ½ä¼šè¿ç§»åˆ° Gymnasium

### 2. è®­ç»ƒæ—¶é—´

- è®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å¤§é‡æ•°æ®æ—¶
- å»ºè®®ä½¿ç”¨ `LightSimBackend` åŠ é€Ÿè®¡ç®—
- ä½¿ç”¨ `MultifolderWithCache` ç¼“å­˜æ•°æ®ä»¥å‡å°‘ I/O å¼€é”€

### 3. å†…å­˜ä½¿ç”¨

- ä¿å­˜ GIF åŠ¨ç”»ä¼šå ç”¨å¤§é‡å†…å­˜ï¼Œè¯„ä¼°æ—¶è°¨æ…ä½¿ç”¨ `save_gif=True`
- å¤§ç½‘ç»œæ¶æ„å’Œé•¿æ—¶é—´è®­ç»ƒå¯èƒ½éœ€è¦æ›´å¤šå†…å­˜

### 4. æ¨¡å‹ä¿å­˜

- è®­ç»ƒæ—¶åŠ¡å¿…æŒ‡å®š `save_path`ï¼Œå¦åˆ™æ¨¡å‹ä¸ä¼šè¢«ä¿å­˜
- æ¨¡å‹ä¿å­˜æ—¶ä¼šåŒæ—¶ä¿å­˜è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´çš„é…ç½®ï¼Œè¯„ä¼°æ—¶å¿…é¡»ä½¿ç”¨ç›¸åŒçš„é…ç½®

### 5. åŠ¨ä½œç©ºé—´é™åˆ¶

- é»˜è®¤åªæ”¯æŒè¿ç»­åŠ¨ä½œï¼ˆ`redispatch`, `curtail`, `set_storage`ï¼‰
- ä¸æ”¯æŒæ‹“æ‰‘æ“ä½œï¼ˆå¼€å…³çº¿è·¯ã€æ”¹å˜å˜ç”µç«™é…ç½®ç­‰ï¼‰
- å¦‚éœ€æ‹“æ‰‘æ“ä½œï¼Œè¯·è€ƒè™‘ä½¿ç”¨å…¶ä»–åŸºçº¿ï¼ˆå¦‚ `ExpertAgent`ï¼‰

## ğŸ” å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç½‘ç»œæ¶æ„ï¼Ÿ

A: å»ºè®®ä» `[200, 200, 200]` å¼€å§‹ï¼Œæ ¹æ®ç¯å¢ƒå¤æ‚åº¦è°ƒæ•´ï¼š
- ç®€å•ç¯å¢ƒï¼š`[100, 100]` æˆ– `[128, 128]`
- å¤æ‚ç¯å¢ƒï¼š`[256, 256, 256]` æˆ– `[512, 512, 256]`

### Q: è®­ç»ƒæ—¶å¦‚ä½•é€‰æ‹©åˆé€‚çš„è§‚å¯Ÿå±æ€§ï¼Ÿ

A: å»ºè®®ä»é»˜è®¤åˆ—è¡¨å¼€å§‹ï¼Œç„¶åæ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´ï¼š
- å¦‚æœåªå…³æ³¨åŠŸç‡å¹³è¡¡ï¼Œå¯ä»¥åªä¿ç•™ `prod_p`, `load_p`, `rho`
- å¦‚æœéœ€è¦æ—¶é—´ä¿¡æ¯ï¼Œä¿ç•™ `day_of_week`, `hour_of_day` ç­‰

### Q: å¦‚ä½•æé«˜è®­ç»ƒæ•ˆç‡ï¼Ÿ

A: 
1. ä½¿ç”¨ `LightSimBackend` æ›¿ä»£é»˜è®¤åç«¯
2. ä½¿ç”¨ `MultifolderWithCache` ç¼“å­˜æ•°æ®
3. è¿‡æ»¤è®­ç»ƒæ•°æ®ï¼Œåªä½¿ç”¨éƒ¨åˆ†åœºæ™¯
4. è°ƒæ•´ `save_every_xxx_steps` å’Œ `eval_every_xxx_steps` ä»¥å‡å°‘ I/O

### Q: è¯„ä¼°ç»“æœä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

A: 
1. å¢åŠ è¯„ä¼°å›åˆæ•° `nb_episode`
2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å……åˆ†è®­ç»ƒ
3. å°è¯•ä¸åŒçš„éšæœºç§å­
4. æ£€æŸ¥ç¯å¢ƒé…ç½®æ˜¯å¦ä¸€è‡´

### Q: å¦‚ä½•ä¸å…¶ä»–åŸºçº¿å¯¹æ¯”ï¼Ÿ

A: å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„å…¶ä»–åŸºçº¿å®ç°ï¼Œå¦‚ï¼š
- `DoNothing`: ä¸æ‰§è¡Œä»»ä½•åŠ¨ä½œçš„åŸºå‡†
- `OptimCVXPY`: åŸºäºä¼˜åŒ–çš„åŸºçº¿
- `ExpertAgent`: åŸºäºè§„åˆ™çš„ä¸“å®¶ç³»ç»Ÿ

## ğŸ“š å‚è€ƒèµ„æº

- **é¡¹ç›®ä»“åº“**: https://github.com/rte-france/l2rpn-baselines
- **Grid2Op æ–‡æ¡£**: https://grid2op.readthedocs.io/
- **Stable Baselines3 æ–‡æ¡£**: https://stable-baselines3.readthedocs.io/
- **L2RPN ç«èµ›**: https://l2rpn.chalearn.org/
- **å®˜æ–¹æ–‡æ¡£**: `docs/ppo_stable_baselines.rst`

## ğŸ“ ç¤ºä¾‹é¡¹ç›®

é¡¹ç›®æ ¹ç›®å½•çš„ `examples/` æ–‡ä»¶å¤¹åŒ…å«å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼š

- `examples/ppo_stable_baselines/`: åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
- `examples/ppo_stable_baselines_idf_2023/`: IDF 2023 ç«èµ›ç¤ºä¾‹

å»ºè®®å‚è€ƒè¿™äº›ç¤ºä¾‹äº†è§£å®Œæ•´çš„å·¥ä½œæµç¨‹ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª Mozilla Public License 2.0 (MPL-2.0)ã€‚

---

**Happy Training! ğŸš€**

